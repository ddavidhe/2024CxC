{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pathlib \n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import date \n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices_df = pd.read_csv('devices.csv')\n",
    "readings_df = pd.read_csv('sampled_readings.csv')\n",
    "reading_types_df = pd.read_csv('reading_types.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection Using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.nanpercentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.nanpercentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        print(\"First Quartertile:\", Q1, \". Third Quartile: \", Q3, \".Interquartile Range: \", IQR)\n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v >= n )\n",
    "    \n",
    "    return multiple_outliers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Quartertile: 428.0 . Third Quartile:  564.8 .Interquartile Range:  136.79999999999995\n",
      "First Quartertile: 0.0 . Third Quartile:  1.7 .Interquartile Range:  1.7\n",
      "First Quartertile: 31.0 . Third Quartile:  249.0 .Interquartile Range:  218.0\n",
      "First Quartertile: 0.0 . Third Quartile:  0.0 .Interquartile Range:  0.0\n",
      "First Quartertile: 0.0 . Third Quartile:  0.2 .Interquartile Range:  0.2\n",
      "First Quartertile: 0.0 . Third Quartile:  0.1 .Interquartile Range:  0.1\n",
      "First Quartertile: 3.8 . Third Quartile:  13.2 .Interquartile Range:  9.399999999999999\n",
      "First Quartertile: 0.0 . Third Quartile:  31.7 .Interquartile Range:  31.7\n",
      "First Quartertile: 18.6 . Third Quartile:  20.0 .Interquartile Range:  1.3999999999999986\n",
      "First Quartertile: 0.0 . Third Quartile:  23.0 .Interquartile Range:  23.0\n",
      "First Quartertile: 20.4 . Third Quartile:  23.6 .Interquartile Range:  3.200000000000003\n",
      "First Quartertile: 27.9 . Third Quartile:  45.1 .Interquartile Range:  17.200000000000003\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 35630376 entries, 1 to 39004223\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   device_id      int64  \n",
      " 1   date           object \n",
      " 2   value_type_id  int64  \n",
      " 3   value          float64\n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 1.3+ GB\n"
     ]
    }
   ],
   "source": [
    "df = readings_df \n",
    "for k, v in readings_df.groupby('value_type_id'):\n",
    "    outliers = detect_outliers(v, 1, ['value'])\n",
    "    df = df.drop(outliers, axis = 0) \n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging devices with sampled readings\n",
    "Since devices in the same building_id are situated in the same environment we should expect that they share similar IAQ. There may be differences depending on the # of people in different rooms but we will hypothesize that the difference is minimal. Here we map device_ids to buildings to group all devices by building_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging devices with sampled readings\n",
    "\n",
    "df = pd.merge(df, devices_df, on='device_id', how='inner')\n",
    "df = df.drop('device_id', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have aggregate value_types with the same hour\n",
    "Since the data is not given in consistent time-steps we will use downsampling to aggregate data points for 5 minute time-steps. We will partition the data based on value_type_id as well as building_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['date'] = df['date'].dt.floor('5min')\n",
    "\n",
    "aggregate_function = {'value': 'mean'}\n",
    "df = df.groupby(['building_id', 'date', 'value_type_id']).agg(aggregate_function)\n",
    "\n",
    "\n",
    "#pivot table so that value_type_id is a column \n",
    "df = pd.pivot_table(df, values = 'value', index = ['date', 'building_id'], columns = 'value_type_id').reset_index()  \n",
    "df = df.rename_axis(None).rename_axis(None, axis=1)\n",
    "df.columns = df.columns.map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolating Small Gaps\n",
    "For small gaps (15 minutes) in data we will use interpolation to predict missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'method' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m cnt \u001b[38;5;241m=\u001b[39m df1[\u001b[38;5;28mstr\u001b[39m(i)]\u001b[38;5;241m.\u001b[39mcount\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cnt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m---> 16\u001b[0m         df1[\u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m df1[\u001b[38;5;28mstr\u001b[39m(i)]\u001b[38;5;241m.\u001b[39minterpolate(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspline\u001b[39m\u001b[38;5;124m'\u001b[39m, order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[43mcnt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m, \u001b[38;5;241m3\u001b[39m), limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \n\u001b[0;32m     18\u001b[0m         df1[\u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m df1[\u001b[38;5;28mstr\u001b[39m(i)]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'method' and 'int'"
     ]
    }
   ],
   "source": [
    "# df = df.sort_values(by = 'date')\n",
    "\n",
    "# def my_interp(x):\n",
    "#     if x.notnull().sum() > 1:\n",
    "#         return x.interpolate(method='nearest').ffill().bfill()\n",
    "#     else:\n",
    "#         return x.ffill().bfill()\n",
    "interp = pd.DataFrame()\n",
    "for building, df1 in df.groupby('building_id'):\n",
    "        df1 = df1.sort_values(by = 'date')\n",
    "        # resampled = df.resample('60min', on = 'date', label = 'left').mean() \n",
    "        # resampled ['date'] = resampled.index.values\n",
    "        for i in range (1, 13): \n",
    "                cnt = df1[str(i)].count()\n",
    "                if cnt != 0: \n",
    "                        df1[str(i)] = df1[str(i)].interpolate(method='spline', order = min(cnt - 1, 3), limit = 3, axis=0)\n",
    "                else:  \n",
    "                        df1[str(i)] = df1[str(i)].fillna(0)\n",
    "        interp = pd.concat([interp, df1], ignore_index = True)\n",
    "# df.info()\n",
    "# df = df.sort_values(by = 'date')\n",
    "# print(df.head(100))\n",
    "# # resampled_df.to_csv('googoogagag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = interp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Hour\n",
    "Since IAQ most likely decreases off-work hours or when there is a lack of personell we will add features to determine working hours and weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour_mapping (1 if between 8am and 6pm)\n",
    "\n",
    "# readings_df['date'] = pd.to_datetime(readings_df['date'])\n",
    "\n",
    "df['work_hours'] = df['date'].dt.hour.between(8, 18)\n",
    "df['work_hours'].map({True: 1, False: 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day of week mapping (1 weekday, 0 weekend)\n",
    "\n",
    "df['day type'] = df['date'].dt.dayofweek.map({\n",
    "    0: 1,\n",
    "    1: 1,\n",
    "    2: 1,\n",
    "    3: 1,\n",
    "    4: 1,\n",
    "    5: 0, \n",
    "    6: 0\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# season mapping, use or not depending on seasonality dicky-fuller test\n",
    "\n",
    "df['season'] = df['date'].dt.month.map({\n",
    "    1: 'Winter',\n",
    "    2: 'Winter',\n",
    "    3: 'Spring',\n",
    "    4: 'Spring',\n",
    "    5: 'Spring',\n",
    "    6: 'Summer',\n",
    "    7: 'Summer',\n",
    "    8: 'Summer',\n",
    "    9: 'Fall',\n",
    "    10: 'Fall',\n",
    "    11: 'Fall',\n",
    "    12: 'Winter'\n",
    "})\n",
    "\n",
    "season_encoder = pd.get_dummies(df['season'])\n",
    "df = df.join(season_encoder)\n",
    "df = df.drop('season', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trimester_day(row): \n",
    "    dt = (row['date']).date()\n",
    "    if 3 <= dt.month <= 5:\n",
    "        return (dt - date(year=dt.year, month=3, day=1)) # Spring\n",
    "    elif 6 <= dt.month <= 8:\n",
    "        return (dt - date(year=dt.year, month=6, day=1))  # Summer\n",
    "    elif 9 <= dt.month <= 11:\n",
    "        return (dt - date(year=dt.year, month=9, day=1))  # Autumn\n",
    "    else:\n",
    "        if(dt.month == 12): \n",
    "            return (dt - date(year=dt.year, month=12, day=1))\n",
    "        return (dt - date(year=dt.year - 1, month=12, day=1))  # Winter\n",
    "    \n",
    "df['trimester_day'] = df.apply(get_trimester_day, axis = 1)\n",
    "df['trimester_day'] = df['trimester_day'].dt.days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = resampled_df \n",
    "# building_encoder = pd.get_dummies(final_df['building_id'], prefix  = 'building')\n",
    "# final_df = final_df.join(building_encoder)\n",
    "# final_df = final_df.drop('building_id', axis = 1) \n",
    "\n",
    "# device_encoder = pd.get_dummies(mergedDf['device_id'], prefix = 'device')\n",
    "# mergedDf = mergedDf.join(device_encoder) don't know if this matters as much "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTANT: uncomment the line highlighted if you do not have preprocessed.csv, use this in the model.ipynb (so we stop working on same file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1832737 entries, 0 to 1832736\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   date           datetime64[ns]\n",
      " 1   building_id    int64         \n",
      " 2   1              float64       \n",
      " 3   2              float64       \n",
      " 4   3              float64       \n",
      " 5   4              float64       \n",
      " 6   5              float64       \n",
      " 7   6              float64       \n",
      " 8   7              float64       \n",
      " 9   8              float64       \n",
      " 10  9              float64       \n",
      " 11  10             float64       \n",
      " 12  11             float64       \n",
      " 13  12             float64       \n",
      " 14  work_hours     bool          \n",
      " 15  day type       int64         \n",
      " 16  Fall           bool          \n",
      " 17  Spring         bool          \n",
      " 18  Summer         bool          \n",
      " 19  Winter         bool          \n",
      " 20  trimester_day  int64         \n",
      "dtypes: bool(5), datetime64[ns](1), float64(12), int64(3)\n",
      "memory usage: 232.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df.to_csv('preprocessed_lstm.csv') # <-- COMMENT THIS OUT IF YOU DON'T HAVE preprocessd.csv yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1832737 entries, 0 to 1832736\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count    Dtype         \n",
      "---  ------         --------------    -----         \n",
      " 0   date           1832737 non-null  datetime64[ns]\n",
      " 1   building_id    1832737 non-null  int64         \n",
      " 2   1              1043716 non-null  float64       \n",
      " 3   2              1074519 non-null  float64       \n",
      " 4   3              1024387 non-null  float64       \n",
      " 5   4              943762 non-null   float64       \n",
      " 6   5              971948 non-null   float64       \n",
      " 7   6              983870 non-null   float64       \n",
      " 8   7              1043221 non-null  float64       \n",
      " 9   8              440684 non-null   float64       \n",
      " 10  9              1006545 non-null  float64       \n",
      " 11  10             598425 non-null   float64       \n",
      " 12  11             1017428 non-null  float64       \n",
      " 13  12             1072212 non-null  float64       \n",
      " 14  work_hours     1832737 non-null  bool          \n",
      " 15  day type       1832737 non-null  int64         \n",
      " 16  Fall           1832737 non-null  bool          \n",
      " 17  Spring         1832737 non-null  bool          \n",
      " 18  Summer         1832737 non-null  bool          \n",
      " 19  Winter         1832737 non-null  bool          \n",
      " 20  trimester_day  1832737 non-null  int64         \n",
      "dtypes: bool(5), datetime64[ns](1), float64(12), int64(3)\n",
      "memory usage: 232.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info(verbose = True, show_counts = True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
