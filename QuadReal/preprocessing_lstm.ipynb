{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from seaborn) (2.2.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from seaborn) (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: statsmodels in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: numpy<2,>=1.18 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from statsmodels) (1.26.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from statsmodels) (1.12.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.0 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from statsmodels) (2.2.1)\n",
      "Requirement already satisfied: patsy>=0.5.4 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from statsmodels) (0.5.6)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from statsmodels) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2024.1)\n",
      "Requirement already satisfied: six in c:\\users\\thoma\\onedrive\\documents\\github\\2024cxc\\quadreal\\.conda\\lib\\site-packages (from patsy>=0.5.4->statsmodels) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install statsmodels\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pathlib \n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import date \n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_path  = os.getcwd()\n",
    "\n",
    "devices_df = pd.read_csv('devices.csv')\n",
    "readings_df = pd.read_csv('sampled_readings.csv')\n",
    "reading_types_df = pd.read_csv('reading_types.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection Using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.nanpercentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.nanpercentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        print(\"First Quartertile:\", Q1, \". Third Quartile: \", Q3, \".Interquartile Range: \", IQR)\n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v >= n )\n",
    "    \n",
    "    return multiple_outliers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Quartertile: 428.0 . Third Quartile:  564.8 .Interquartile Range:  136.79999999999995\n"
     ]
    }
   ],
   "source": [
    "df = readings_df \n",
    "for k, v in readings_df.groupby('value_type_id'):\n",
    "    outliers = detect_outliers(v, 1, ['value'])\n",
    "    df = df.drop(outliers, axis = 0) \n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging devices with sampled readings\n",
    "Since devices in the same building_id are situated in the same environment we should expect that they share similar IAQ. There may be differences depending on the # of people in different rooms but we will hypothesize that the difference is minimal. Here we map device_ids to buildings to group all devices by building_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging devices with sampled readings\n",
    "\n",
    "df = pd.merge(df, devices_df, on='device_id', how='inner')\n",
    "df = df.drop('device_id', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have aggregate value_types with the same hour\n",
    "Since the data is not given in consistent time-steps we will use downsampling to aggregate data points for 5 minute time-steps. We will partition the data based on value_type_id as well as building_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\AppData\\Local\\Temp\\ipykernel_32548\\1082544091.py:2: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df['date'] = df['date'].dt.floor('H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 168254 entries, 0 to 168253\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   date         168254 non-null  datetime64[ns]\n",
      " 1   building_id  168254 non-null  int64         \n",
      " 2   1            159665 non-null  float64       \n",
      " 3   2            163230 non-null  float64       \n",
      " 4   3            157071 non-null  float64       \n",
      " 5   4            137857 non-null  float64       \n",
      " 6   5            150672 non-null  float64       \n",
      " 7   6            151568 non-null  float64       \n",
      " 8   7            157086 non-null  float64       \n",
      " 9   8            51631 non-null   float64       \n",
      " 10  9            159798 non-null  float64       \n",
      " 11  10           95052 non-null   float64       \n",
      " 12  11           156098 non-null  float64       \n",
      " 13  12           163030 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(12), int64(1)\n",
      "memory usage: 18.0 MB\n"
     ]
    }
   ],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['date'] = df['date'].dt.floor('5min')\n",
    "\n",
    "aggregate_function = {'value': 'mean'}\n",
    "df = df.groupby(['building_id', 'date', 'value_type_id']).agg(aggregate_function)\n",
    "\n",
    "\n",
    "#pivot table so that value_type_id is a column \n",
    "df = pd.pivot_table(df, values = 'value', index = ['date', 'building_id'], columns = 'value_type_id').reset_index()  \n",
    "df = df.rename_axis(None).rename_axis(None, axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling readings to hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sort_values(by = 'date')\n",
    "\n",
    "# def my_interp(x):\n",
    "#     if x.notnull().sum() > 1:\n",
    "#         return x.interpolate(method='nearest').ffill().bfill()\n",
    "#     else:\n",
    "#         return x.ffill().bfill()\n",
    "    \n",
    "# for building, df1 in df.groupby('building_id'):\n",
    "#     df1 = df1.sort_values(by = 'date')\n",
    "#     # resampled = df.resample('60min', on = 'date', label = 'left').mean() \n",
    "#     # resampled ['date'] = resampled.index.values\n",
    "#     resampled = df.interpolate(method='linear', axis=0).ffill().bfill()\n",
    "#     resampled_df = pd.concat([resampled_df, resampled], ignore_index = True)\n",
    "\n",
    "# df.info()\n",
    "# df = df.sort_values(by = 'date')\n",
    "# print(df.head(100))\n",
    "# # resampled_df.to_csv('googoogagag.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Hour\n",
    "Since IAQ most likely decreases off-work hours or when there is a lack of personell we will add features to determine working hours and weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "168249    0\n",
       "168250    0\n",
       "168251    0\n",
       "168252    0\n",
       "168253    0\n",
       "Name: work_hours, Length: 168254, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hour_mapping (1 if between 8am and 6pm)\n",
    "\n",
    "# readings_df['date'] = pd.to_datetime(readings_df['date'])\n",
    "\n",
    "df['work_hours'] = df['date'].dt.hour.between(8, 18)\n",
    "df['work_hours'].map({True: 1, False: 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 168254 entries, 0 to 168253\n",
      "Data columns (total 16 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   date         168254 non-null  datetime64[ns]\n",
      " 1   building_id  168254 non-null  int64         \n",
      " 2   1            159665 non-null  float64       \n",
      " 3   2            163230 non-null  float64       \n",
      " 4   3            157071 non-null  float64       \n",
      " 5   4            137857 non-null  float64       \n",
      " 6   5            150672 non-null  float64       \n",
      " 7   6            151568 non-null  float64       \n",
      " 8   7            157086 non-null  float64       \n",
      " 9   8            51631 non-null   float64       \n",
      " 10  9            159798 non-null  float64       \n",
      " 11  10           95052 non-null   float64       \n",
      " 12  11           156098 non-null  float64       \n",
      " 13  12           163030 non-null  float64       \n",
      " 14  work_hours   168254 non-null  bool          \n",
      " 15  day type     168254 non-null  int64         \n",
      "dtypes: bool(1), datetime64[ns](1), float64(12), int64(2)\n",
      "memory usage: 19.4 MB\n"
     ]
    }
   ],
   "source": [
    "# day of week mapping (1 weekday, 0 weekend)\n",
    "\n",
    "df['day type'] = df['date'].dt.dayofweek.map({\n",
    "    0: 1,\n",
    "    1: 1,\n",
    "    2: 1,\n",
    "    3: 1,\n",
    "    4: 1,\n",
    "    5: 0, \n",
    "    6: 0\n",
    "})\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# season mapping, use or not depending on seasonality dicky-fuller test\n",
    "\n",
    "df['season'] = df['date'].dt.month.map({\n",
    "    1: 'Winter',\n",
    "    2: 'Winter',\n",
    "    3: 'Spring',\n",
    "    4: 'Spring',\n",
    "    5: 'Spring',\n",
    "    6: 'Summer',\n",
    "    7: 'Summer',\n",
    "    8: 'Summer',\n",
    "    9: 'Fall',\n",
    "    10: 'Fall',\n",
    "    11: 'Fall',\n",
    "    12: 'Winter'\n",
    "})\n",
    "\n",
    "season_encoder = pd.get_dummies(df['season'])\n",
    "df = df.join(season_encoder)\n",
    "df = df.drop('season', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trimester_day(row): \n",
    "    dt = (row['date']).date()\n",
    "    if 3 <= dt.month <= 5:\n",
    "        return (dt - date(year=dt.year, month=3, day=1)) # Spring\n",
    "    elif 6 <= dt.month <= 8:\n",
    "        return (dt - date(year=dt.year, month=6, day=1))  # Summer\n",
    "    elif 9 <= dt.month <= 11:\n",
    "        return (dt - date(year=dt.year, month=9, day=1))  # Autumn\n",
    "    else:\n",
    "        if(dt.month == 12): \n",
    "            return (dt - date(year=dt.year, month=12, day=1))\n",
    "        return (dt - date(year=dt.year - 1, month=12, day=1))  # Winter\n",
    "    \n",
    "df['trimester_day'] = df.apply(get_trimester_day, axis = 1)\n",
    "df['trimester_day'] = df['trimester_day'].dt.days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = resampled_df \n",
    "# building_encoder = pd.get_dummies(final_df['building_id'], prefix  = 'building')\n",
    "# final_df = final_df.join(building_encoder)\n",
    "# final_df = final_df.drop('building_id', axis = 1) \n",
    "\n",
    "# device_encoder = pd.get_dummies(mergedDf['device_id'], prefix = 'device')\n",
    "# mergedDf = mergedDf.join(device_encoder) don't know if this matters as much "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTANT: uncomment the line highlighted if you do not have preprocessed.csv, use this in the model.ipynb (so we stop working on same file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('preprocessed_lstm.csv') # <-- COMMENT THIS OUT IF YOU DON'T HAVE preprocessd.csv yet \n",
    "print(df.info(verbose = True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
