{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pathlib \n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import date \n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices_df = pd.read_csv('devices.csv')\n",
    "readings_df = pd.read_csv('sampled_readings.csv')\n",
    "reading_types_df = pd.read_csv('reading_types.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection Using IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.nanpercentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.nanpercentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        print(\"First Quartertile:\", Q1, \". Third Quartile: \", Q3, \".Interquartile Range: \", IQR)\n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v >= n )\n",
    "    \n",
    "    return multiple_outliers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Quartertile: 428.0 . Third Quartile:  564.8 .Interquartile Range:  136.79999999999995\n",
      "First Quartertile: 0.0 . Third Quartile:  1.7 .Interquartile Range:  1.7\n",
      "First Quartertile: 31.0 . Third Quartile:  249.0 .Interquartile Range:  218.0\n",
      "First Quartertile: 0.0 . Third Quartile:  0.0 .Interquartile Range:  0.0\n",
      "First Quartertile: 0.0 . Third Quartile:  0.2 .Interquartile Range:  0.2\n",
      "First Quartertile: 0.0 . Third Quartile:  0.1 .Interquartile Range:  0.1\n",
      "First Quartertile: 3.8 . Third Quartile:  13.2 .Interquartile Range:  9.399999999999999\n",
      "First Quartertile: 0.0 . Third Quartile:  31.7 .Interquartile Range:  31.7\n",
      "First Quartertile: 18.6 . Third Quartile:  20.0 .Interquartile Range:  1.3999999999999986\n",
      "First Quartertile: 0.0 . Third Quartile:  23.0 .Interquartile Range:  23.0\n",
      "First Quartertile: 20.4 . Third Quartile:  23.6 .Interquartile Range:  3.200000000000003\n",
      "First Quartertile: 27.9 . Third Quartile:  45.1 .Interquartile Range:  17.200000000000003\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 35630376 entries, 1 to 39004223\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   device_id      int64  \n",
      " 1   date           object \n",
      " 2   value_type_id  int64  \n",
      " 3   value          float64\n",
      "dtypes: float64(1), int64(2), object(1)\n",
      "memory usage: 1.3+ GB\n"
     ]
    }
   ],
   "source": [
    "df = readings_df \n",
    "for k, v in readings_df.groupby('value_type_id'):\n",
    "    outliers = detect_outliers(v, 1, ['value'])\n",
    "    df = df.drop(outliers, axis = 0) \n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging devices with sampled readings\n",
    "Since devices in the same building_id are situated in the same environment we should expect that they share similar IAQ. There may be differences depending on the # of people in different rooms but we will hypothesize that the difference is minimal. Here we map device_ids to buildings to group all devices by building_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging devices with sampled readings\n",
    "\n",
    "df = pd.merge(df, devices_df, on='device_id', how='inner')\n",
    "df = df.drop('device_id', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have aggregate value_types with the same hour\n",
    "Since the data is not given in consistent time-steps we will use downsampling to aggregate data points for 5 minute time-steps. We will partition the data based on value_type_id as well as building_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['date'] = df['date'].dt.floor('5min')\n",
    "\n",
    "aggregate_function = {'value': 'mean'}\n",
    "df = df.groupby(['building_id', 'date', 'value_type_id']).agg(aggregate_function)\n",
    "\n",
    "\n",
    "#pivot table so that value_type_id is a column \n",
    "df = pd.pivot_table(df, values = 'value', index = ['date', 'building_id'], columns = 'value_type_id').reset_index()  \n",
    "df = df.rename_axis(None).rename_axis(None, axis=1)\n",
    "df.columns = df.columns.map(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolating Small Gaps\n",
    "For small gaps (15 minutes) in data we will use interpolation to predict missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "order needs to be specified and greater than 0; got order: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m cnt \u001b[38;5;241m=\u001b[39m df1[\u001b[38;5;28mstr\u001b[39m(i)]\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cnt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m---> 16\u001b[0m         df1[\u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m \u001b[43mdf1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcnt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \n\u001b[0;32m     18\u001b[0m         df1[\u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m df1[\u001b[38;5;28mstr\u001b[39m(i)]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:8206\u001b[0m, in \u001b[0;36mNDFrame.interpolate\u001b[1;34m(self, method, axis, limit, inplace, limit_direction, limit_area, downcast, **kwargs)\u001b[0m\n\u001b[0;32m   8204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   8205\u001b[0m     index \u001b[38;5;241m=\u001b[39m missing\u001b[38;5;241m.\u001b[39mget_interp_index(method, obj\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m-> 8206\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit_area\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_area\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8212\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdowncast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdowncast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   8217\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   8218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_transpose:\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\base.py:265\u001b[0m, in \u001b[0;36mDataManager.interpolate\u001b[1;34m(self, inplace, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minterpolate\u001b[39m(\u001b[38;5;28mself\u001b[39m, inplace: \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minterpolate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:354\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 354\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    357\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1507\u001b[0m, in \u001b[0;36mBlock.interpolate\u001b[1;34m(self, method, index, inplace, limit, limit_direction, limit_area, downcast, using_cow, **kwargs)\u001b[0m\n\u001b[0;32m   1504\u001b[0m copy, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_refs_and_copy(using_cow, inplace)\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;66;03m# Dispatch to the EA method.\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit_area\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_area\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1515\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1517\u001b[0m data \u001b[38;5;241m=\u001b[39m extract_array(new_values, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1519\u001b[0m nb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_block_same_class(data, refs\u001b[38;5;241m=\u001b[39mrefs)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\arrays\\numpy_.py:297\u001b[0m, in \u001b[0;36mNumpyExtensionArray.interpolate\u001b[1;34m(self, method, axis, index, limit, limit_direction, limit_area, copy, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     out_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ndarray\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# TODO: assert we have floating dtype?\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m \u001b[43mmissing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate_2d_inplace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit_area\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_area\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copy:\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\missing.py:370\u001b[0m, in \u001b[0;36minterpolate_2d_inplace\u001b[1;34m(data, index, axis, method, limit, limit_direction, limit_area, fill_value, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m     _interpolate_1d(\n\u001b[0;32m    353\u001b[0m         indices\u001b[38;5;241m=\u001b[39mindices,\n\u001b[0;32m    354\u001b[0m         yvalues\u001b[38;5;241m=\u001b[39myvalues,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    362\u001b[0m     )\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"apply_along_axis\" has incompatible type\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# \"Callable[[ndarray[Any, Any]], None]\"; expected \"Callable[...,\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# Union[_SupportsArray[dtype[<nothing>]], Sequence[_SupportsArray\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# [dtype[<nothing>]]], Sequence[Sequence[_SupportsArray[dtype[<nothing>]]]],\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;66;03m# Sequence[Sequence[Sequence[_SupportsArray[dtype[<nothing>]]]]],\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Sequence[Sequence[Sequence[Sequence[_SupportsArray[dtype[<nothing>]]]]]]]]\"\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\shape_base.py:379\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot apply_along_axis when any iteration dimensions are 0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    378\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m res \u001b[38;5;241m=\u001b[39m asanyarray(\u001b[43mfunc1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minarr_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43mind0\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# build a buffer for storing evaluations of func1d.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# remove the requested axis, and add the new ones on the end.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# laid out so that each write is contiguous.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# for a tuple index inds, buff[inds] = func1d(inarr_view[inds])\u001b[39;00m\n\u001b[0;32m    385\u001b[0m buff \u001b[38;5;241m=\u001b[39m zeros(inarr_view\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m res\u001b[38;5;241m.\u001b[39mshape, res\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\missing.py:352\u001b[0m, in \u001b[0;36minterpolate_2d_inplace.<locals>.func\u001b[1;34m(yvalues)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(yvalues: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;66;03m# process 1-d slices in the axis direction\u001b[39;00m\n\u001b[1;32m--> 352\u001b[0m     \u001b[43m_interpolate_1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43myvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit_direction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit_area\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_area_validated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbounds_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\missing.py:485\u001b[0m, in \u001b[0;36m_interpolate_1d\u001b[1;34m(indices, yvalues, method, limit, limit_direction, limit_area, fill_value, bounds_error, order, **kwargs)\u001b[0m\n\u001b[0;32m    481\u001b[0m     yvalues[invalid] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minterp(\n\u001b[0;32m    482\u001b[0m         indices[invalid], indices[valid][indexer], yvalues[valid][indexer]\n\u001b[0;32m    483\u001b[0m     )\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 485\u001b[0m     yvalues[invalid] \u001b[38;5;241m=\u001b[39m \u001b[43m_interpolate_scipy_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43myvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43minvalid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbounds_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datetimelike:\n\u001b[0;32m    497\u001b[0m     yvalues[preserve_nans] \u001b[38;5;241m=\u001b[39m NaT\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\missing.py:555\u001b[0m, in \u001b[0;36m_interpolate_scipy_wrapper\u001b[1;34m(x, y, new_x, method, fill_value, bounds_error, order, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspline\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;66;03m# GH #10633, #24014\u001b[39;00m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isna(order) \u001b[38;5;129;01mor\u001b[39;00m (order \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 555\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    556\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder needs to be specified and greater than 0; got order: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    557\u001b[0m         )\n\u001b[0;32m    558\u001b[0m     terp \u001b[38;5;241m=\u001b[39m interpolate\u001b[38;5;241m.\u001b[39mUnivariateSpline(x, y, k\u001b[38;5;241m=\u001b[39morder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    559\u001b[0m     new_y \u001b[38;5;241m=\u001b[39m terp(new_x)\n",
      "\u001b[1;31mValueError\u001b[0m: order needs to be specified and greater than 0; got order: 0"
     ]
    }
   ],
   "source": [
    "# df = df.sort_values(by = 'date')\n",
    "\n",
    "# def my_interp(x):\n",
    "#     if x.notnull().sum() > 1:\n",
    "#         return x.interpolate(method='nearest').ffill().bfill()\n",
    "#     else:\n",
    "#         return x.ffill().bfill()\n",
    "interp = pd.DataFrame()\n",
    "for building, df1 in df.groupby('building_id'):\n",
    "        df1 = df1.sort_values(by = 'date')\n",
    "        # resampled = df.resample('60min', on = 'date', label = 'left').mean() \n",
    "        # resampled ['date'] = resampled.index.values\n",
    "        for i in range (1, 13): \n",
    "                cnt = df1[str(i)].count()\n",
    "                if cnt > 1: \n",
    "                        df1[str(i)] = df1[str(i)].interpolate(method='spline', order = min(cnt - 1, 3), limit = 3, axis=0)\n",
    "                elif cnt == 1: \n",
    "                        df1[str(i)] = df1[str(i)].interpolate(method='linear', limit = 3, axis=0)\n",
    "                else:  \n",
    "                        df1[str(i)] = df1[str(i)].fillna(0)\n",
    "        interp = pd.concat([interp, df1], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = interp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Hour\n",
    "Since IAQ most likely decreases off-work hours or when there is a lack of personell we will add features to determine working hours and weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour_mapping (1 if between 8am and 6pm)\n",
    "\n",
    "# readings_df['date'] = pd.to_datetime(readings_df['date'])\n",
    "\n",
    "df['work_hours'] = df['date'].dt.hour.between(8, 18)\n",
    "df['work_hours'].map({True: 1, False: 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day of week mapping (1 weekday, 0 weekend)\n",
    "\n",
    "df['day type'] = df['date'].dt.dayofweek.map({\n",
    "    0: 1,\n",
    "    1: 1,\n",
    "    2: 1,\n",
    "    3: 1,\n",
    "    4: 1,\n",
    "    5: 0, \n",
    "    6: 0\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# season mapping, use or not depending on seasonality dicky-fuller test\n",
    "\n",
    "df['season'] = df['date'].dt.month.map({\n",
    "    1: 'Winter',\n",
    "    2: 'Winter',\n",
    "    3: 'Spring',\n",
    "    4: 'Spring',\n",
    "    5: 'Spring',\n",
    "    6: 'Summer',\n",
    "    7: 'Summer',\n",
    "    8: 'Summer',\n",
    "    9: 'Fall',\n",
    "    10: 'Fall',\n",
    "    11: 'Fall',\n",
    "    12: 'Winter'\n",
    "})\n",
    "\n",
    "season_encoder = pd.get_dummies(df['season'])\n",
    "df = df.join(season_encoder)\n",
    "df = df.drop('season', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trimester_day(row): \n",
    "    dt = (row['date']).date()\n",
    "    if 3 <= dt.month <= 5:\n",
    "        return (dt - date(year=dt.year, month=3, day=1)) # Spring\n",
    "    elif 6 <= dt.month <= 8:\n",
    "        return (dt - date(year=dt.year, month=6, day=1))  # Summer\n",
    "    elif 9 <= dt.month <= 11:\n",
    "        return (dt - date(year=dt.year, month=9, day=1))  # Autumn\n",
    "    else:\n",
    "        if(dt.month == 12): \n",
    "            return (dt - date(year=dt.year, month=12, day=1))\n",
    "        return (dt - date(year=dt.year - 1, month=12, day=1))  # Winter\n",
    "    \n",
    "df['trimester_day'] = df.apply(get_trimester_day, axis = 1)\n",
    "df['trimester_day'] = df['trimester_day'].dt.days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = resampled_df \n",
    "# building_encoder = pd.get_dummies(final_df['building_id'], prefix  = 'building')\n",
    "# final_df = final_df.join(building_encoder)\n",
    "# final_df = final_df.drop('building_id', axis = 1) \n",
    "\n",
    "# device_encoder = pd.get_dummies(mergedDf['device_id'], prefix = 'device')\n",
    "# mergedDf = mergedDf.join(device_encoder) don't know if this matters as much "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTANT: uncomment the line highlighted if you do not have preprocessed.csv, use this in the model.ipynb (so we stop working on same file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1832737 entries, 0 to 1832736\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   date           datetime64[ns]\n",
      " 1   building_id    int64         \n",
      " 2   1              float64       \n",
      " 3   2              float64       \n",
      " 4   3              float64       \n",
      " 5   4              float64       \n",
      " 6   5              float64       \n",
      " 7   6              float64       \n",
      " 8   7              float64       \n",
      " 9   8              float64       \n",
      " 10  9              float64       \n",
      " 11  10             float64       \n",
      " 12  11             float64       \n",
      " 13  12             float64       \n",
      " 14  work_hours     bool          \n",
      " 15  day type       int64         \n",
      " 16  Fall           bool          \n",
      " 17  Spring         bool          \n",
      " 18  Summer         bool          \n",
      " 19  Winter         bool          \n",
      " 20  trimester_day  int64         \n",
      "dtypes: bool(5), datetime64[ns](1), float64(12), int64(3)\n",
      "memory usage: 232.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df.to_csv('preprocessed_lstm.csv') # <-- COMMENT THIS OUT IF YOU DON'T HAVE preprocessd.csv yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1832737 entries, 0 to 1832736\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count    Dtype         \n",
      "---  ------         --------------    -----         \n",
      " 0   date           1832737 non-null  datetime64[ns]\n",
      " 1   building_id    1832737 non-null  int64         \n",
      " 2   1              1043716 non-null  float64       \n",
      " 3   2              1074519 non-null  float64       \n",
      " 4   3              1024387 non-null  float64       \n",
      " 5   4              943762 non-null   float64       \n",
      " 6   5              971948 non-null   float64       \n",
      " 7   6              983870 non-null   float64       \n",
      " 8   7              1043221 non-null  float64       \n",
      " 9   8              440684 non-null   float64       \n",
      " 10  9              1006545 non-null  float64       \n",
      " 11  10             598425 non-null   float64       \n",
      " 12  11             1017428 non-null  float64       \n",
      " 13  12             1072212 non-null  float64       \n",
      " 14  work_hours     1832737 non-null  bool          \n",
      " 15  day type       1832737 non-null  int64         \n",
      " 16  Fall           1832737 non-null  bool          \n",
      " 17  Spring         1832737 non-null  bool          \n",
      " 18  Summer         1832737 non-null  bool          \n",
      " 19  Winter         1832737 non-null  bool          \n",
      " 20  trimester_day  1832737 non-null  int64         \n",
      "dtypes: bool(5), datetime64[ns](1), float64(12), int64(3)\n",
      "memory usage: 232.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info(verbose = True, show_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2023-01-01 00:00:00', '2023-01-01 00:05:00',\n",
      "               '2023-01-01 00:10:00', '2023-01-01 00:15:00',\n",
      "               '2023-01-01 00:20:00', '2023-01-01 00:25:00',\n",
      "               '2023-01-01 00:30:00', '2023-01-01 00:35:00',\n",
      "               '2023-01-01 00:40:00', '2023-01-01 00:45:00',\n",
      "               ...\n",
      "               '2023-12-30 23:15:00', '2023-12-30 23:20:00',\n",
      "               '2023-12-30 23:25:00', '2023-12-30 23:30:00',\n",
      "               '2023-12-30 23:35:00', '2023-12-30 23:40:00',\n",
      "               '2023-12-30 23:45:00', '2023-12-30 23:50:00',\n",
      "               '2023-12-30 23:55:00', '2023-12-31 00:00:00'],\n",
      "              dtype='datetime64[ns]', length=104833, freq='5T')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('preprocessed_lstm.csv')\n",
    "idx = pd.date_range('01-01-2023', '12-31-2023', freq = '5min')\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('date', inplace = True, drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Unnamed: 0                 date  building_id          1  \\\n",
      "date                                                                           \n",
      "2023-01-01 00:00:00       89691  2023-01-01 00:00:00            2        NaN   \n",
      "2023-01-01 00:10:00       89692  2023-01-01 00:10:00            2        NaN   \n",
      "2023-01-01 00:15:00       89693  2023-01-01 00:15:00            2        NaN   \n",
      "2023-01-01 00:20:00       89694  2023-01-01 00:20:00            2  488.30000   \n",
      "2023-01-01 00:25:00       89695  2023-01-01 00:25:00            2  488.05744   \n",
      "...                         ...                  ...          ...        ...   \n",
      "2023-12-31 23:35:00     1832732  2023-12-31 23:35:00           41  412.00000   \n",
      "2023-12-31 23:40:00     1832733  2023-12-31 23:40:00           41  415.81025   \n",
      "2023-12-31 23:45:00     1832734  2023-12-31 23:45:00           41  432.00000   \n",
      "2023-12-31 23:50:00     1832735  2023-12-31 23:50:00           41  432.00000   \n",
      "2023-12-31 23:55:00     1832736  2023-12-31 23:55:00           41  410.00000   \n",
      "\n",
      "                            2           3    4         5         6          7  \\\n",
      "date                                                                            \n",
      "2023-01-01 00:00:00  0.000000         NaN  NaN       NaN       NaN        NaN   \n",
      "2023-01-01 00:10:00 -0.002883  436.000000  NaN       NaN  0.100000        NaN   \n",
      "2023-01-01 00:15:00  0.000000  426.422302  NaN       NaN  0.099941        NaN   \n",
      "2023-01-01 00:20:00 -0.002882  417.000000  NaN       NaN  0.099941        NaN   \n",
      "2023-01-01 00:25:00  0.000000  407.607770  NaN  0.200000  0.099941        NaN   \n",
      "...                       ...         ...  ...       ...       ...        ...   \n",
      "2023-12-31 23:35:00  1.310782  317.000000  0.0  0.000000  0.026423   8.400000   \n",
      "2023-12-31 23:40:00  0.000000  236.090227  0.0  0.150000       NaN   1.700000   \n",
      "2023-12-31 23:45:00  1.650000  151.500000  0.0  0.086587       NaN -23.380272   \n",
      "2023-12-31 23:50:00  1.700000  239.500000  0.0  0.086596       NaN -35.543393   \n",
      "2023-12-31 23:55:00  1.650000  566.000000  0.0  0.300000       NaN   5.766667   \n",
      "\n",
      "                     ...   10         11         12  work_hours  day type  \\\n",
      "date                 ...                                                    \n",
      "2023-01-01 00:00:00  ...  0.0  18.100000  33.500000       False         0   \n",
      "2023-01-01 00:10:00  ...  0.0  17.528512  33.580171       False         0   \n",
      "2023-01-01 00:15:00  ...  0.0  17.538262  33.400000       False         0   \n",
      "2023-01-01 00:20:00  ...  0.0  18.100000  33.588936       False         0   \n",
      "2023-01-01 00:25:00  ...  0.0  17.558299  33.593114       False         0   \n",
      "...                  ...  ...        ...        ...         ...       ...   \n",
      "2023-12-31 23:35:00  ...  NaN  17.837281  39.900000       False         0   \n",
      "2023-12-31 23:40:00  ...  NaN  17.562409  38.300000       False         0   \n",
      "2023-12-31 23:45:00  ...  NaN  18.100000  47.100000       False         0   \n",
      "2023-12-31 23:50:00  ...  NaN  16.600000  47.000000       False         0   \n",
      "2023-12-31 23:55:00  ...  NaN  19.200000  41.700000       False         0   \n",
      "\n",
      "                      Fall  Spring  Summer  Winter  trimester_day  \n",
      "date                                                               \n",
      "2023-01-01 00:00:00  False   False   False    True             31  \n",
      "2023-01-01 00:10:00  False   False   False    True             31  \n",
      "2023-01-01 00:15:00  False   False   False    True             31  \n",
      "2023-01-01 00:20:00  False   False   False    True             31  \n",
      "2023-01-01 00:25:00  False   False   False    True             31  \n",
      "...                    ...     ...     ...     ...            ...  \n",
      "2023-12-31 23:35:00  False   False   False    True             30  \n",
      "2023-12-31 23:40:00  False   False   False    True             30  \n",
      "2023-12-31 23:45:00  False   False   False    True             30  \n",
      "2023-12-31 23:50:00  False   False   False    True             30  \n",
      "2023-12-31 23:55:00  False   False   False    True             30  \n",
      "\n",
      "[1740938 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "df = df.reindex(idx, fill_value = np.nan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
