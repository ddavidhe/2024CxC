{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit #for data preprocessing and crass validating \n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression #logistic Regression\n",
    "from sklearn.ensemble import RandomForestRegressor #Random Forest \n",
    "\n",
    "from statistics import mean\n",
    "from hyperopt import Trials, hp, fmin, tpe, STATUS_OK, space_eval #for hyperparameter tuning and minimizing\n",
    "\n",
    "from cyclic_boosting.pipelines import pipeline_CBClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "import itertools\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "from scipy.stats import boxcox \n",
    "from scipy.special import inv_boxcox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_csv('preprocessed_lstm.csv')\n",
    "samples['date'] = pd.to_datetime(samples['date'])\n",
    "reading_types = pd.read_csv('reading_types.csv')\n",
    "# samples.info()\n",
    "\n",
    "df_lst = [(k, v) for k, v in samples.groupby('building_id')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX shape == (3705, 10, 12).\n",
      "trainY shape == (3705, 12).\n",
      "Epoch 1/15\n",
      "47/47 [==============================] - 7s 47ms/step - loss: 0.2417 - val_loss: 0.3254\n",
      "Epoch 2/15\n",
      "47/47 [==============================] - 1s 23ms/step - loss: 0.1623 - val_loss: 0.3353\n",
      "Epoch 3/15\n",
      "47/47 [==============================] - 1s 23ms/step - loss: 0.1434 - val_loss: 0.3398\n",
      "Epoch 4/15\n",
      "47/47 [==============================] - 1s 24ms/step - loss: 0.1315 - val_loss: 0.3394\n",
      "Epoch 5/15\n",
      "47/47 [==============================] - 1s 26ms/step - loss: 0.1212 - val_loss: 0.3293\n",
      "Epoch 6/15\n",
      "47/47 [==============================] - 1s 24ms/step - loss: 0.1109 - val_loss: 0.3162\n",
      "Epoch 7/15\n",
      "47/47 [==============================] - 1s 25ms/step - loss: 0.1070 - val_loss: 0.2988\n",
      "Epoch 8/15\n",
      "47/47 [==============================] - 1s 24ms/step - loss: 0.1036 - val_loss: 0.2765\n",
      "Epoch 9/15\n",
      "47/47 [==============================] - 1s 24ms/step - loss: 0.1016 - val_loss: 0.2579\n",
      "Epoch 10/15\n",
      "47/47 [==============================] - 1s 25ms/step - loss: 0.0999 - val_loss: 0.2428\n",
      "Epoch 11/15\n",
      "47/47 [==============================] - 1s 25ms/step - loss: 0.0996 - val_loss: 0.2294\n",
      "Epoch 12/15\n",
      "47/47 [==============================] - 1s 24ms/step - loss: 0.0985 - val_loss: 0.2343\n",
      "Epoch 13/15\n",
      "47/47 [==============================] - 1s 24ms/step - loss: 0.0991 - val_loss: 0.2595\n",
      "Epoch 14/15\n",
      "47/47 [==============================] - 1s 24ms/step - loss: 0.0979 - val_loss: 0.2431\n",
      "Epoch 15/15\n",
      "47/47 [==============================] - 1s 25ms/step - loss: 0.0980 - val_loss: 0.2559\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Thomas\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Thomas\\AppData\\Local\\Temp\\ipykernel_27924\\3478894849.py\", line 62, in <module>\n",
      "    history = model.fit(trainX, trainY, batch_size=64, validation_split = 0.2, epochs=15, verbose = 1, callbacks = [early_stop], warm_start = True)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"c:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 67, in error_handler\n",
      "    filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: Model.fit() got an unexpected keyword argument 'warm_start'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Thomas\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Thomas\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Thomas\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Thomas\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Thomas\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Thomas\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1160, in get_records\n",
      "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Thomas\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"C:\\Users\\Thomas\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 83, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"C:\\Users\\Thomas\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 587, in mapper\n",
      "    return cls(f, options)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Thomas\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 551, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Thomas\\AppData\\Roaming\\Python\\Python311\\site-packages\\executing\\executing.py\", line 283, in executing\n",
      "    assert_(new_stmts <= stmts)\n",
      "  File \"C:\\Users\\Thomas\\AppData\\Roaming\\Python\\Python311\\site-packages\\executing\\executing.py\", line 80, in assert_\n",
      "    raise AssertionError(str(message))\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "value_type_ids = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
    "\n",
    "for building, df in df_lst[0:1]:\n",
    "    for typeId in value_type_ids:\n",
    "        train_dates = pd.to_datetime(df['date'])\n",
    "        multivariate = df.drop(['Unnamed: 0', 'building_id', 'date'], axis = 1)  \n",
    "        # multivariate.info()\n",
    "        multivariate = multivariate.astype('float32')\n",
    "        multivariate = multivariate[['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']]\n",
    "\n",
    "        # boxcox\n",
    "        # multivariate[typeId] = multivariate[typeId].apply(lambda x: x + 1e-6)\n",
    "        # df_boxcox_vals, fitted_lambda = boxcox(multivariate[typeId])\n",
    "        # multivariate[typeId] = df_boxcox_vals\n",
    "\n",
    "        sz = len(multivariate)\n",
    "        train_sz = int(sz * 0.5)\n",
    "        test_sz = len(multivariate) - train_sz\n",
    "\n",
    "        scaler = StandardScaler()  \n",
    "        scaler = scaler.fit(multivariate) \n",
    "        df_scaled = scaler.transform(multivariate)\n",
    "\n",
    "        train, test = df_scaled[0:train_sz,:], df_scaled[train_sz:sz,:]\n",
    "\n",
    "        def create_dataset(dataset, look_back = 1):\n",
    "            dataX, dataY = [], []\n",
    "            for i in range(len(dataset) - look_back - 1):\n",
    "                a = dataset[i:(i + look_back), :]\n",
    "                dataX.append(a)\n",
    "                dataY.append(dataset[i + look_back, :])\n",
    "            return np.array(dataX), np.array(dataY)\n",
    "        \n",
    "        look_back = 10\n",
    "        trainX, trainY = create_dataset(train, look_back)\n",
    "        testX, testY = create_dataset(test, look_back)\n",
    "        \n",
    "        # reshape input to be [samples, time steps, features]\n",
    "        trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "\n",
    "        print('trainX shape == {}.'.format(trainX.shape))\n",
    "        print('trainY shape == {}.'.format(trainY.shape)) \n",
    "\n",
    "        # x_train, x_test, y_train, y_test = train_test_split(df.drop(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], axis = 1), df[['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']])\n",
    "        # x_train = np.asarray(x_train).astype(np.float32)\n",
    "        # y_train = np.asarray(y_train).astype(np.float32)\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.LSTM(150,input_shape = (trainX.shape[1], trainX.shape[2]), return_sequences = True))\n",
    "        model.add(LSTM(50, return_sequences = False))\n",
    "        # model.add(LSTM(100))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dense(12, activation= 'linear'))\n",
    "\n",
    "\n",
    "        early_stop = EarlyStopping(monitor = 'val_loss', patience = 5)\n",
    "\n",
    "        model.compile(optimizer= tf.keras.optimizers.Adam(), loss= tf.keras.losses.Huber())\n",
    "\n",
    "        history = model.fit(trainX, trainY, batch_size=64, validation_split = 0.2, epochs=15, verbose = 1)\n",
    "        history = model.fit(trainX, trainY, batch_size=64, validation_split = 0.2, epochs=15, verbose = 1, callbacks = [early_stop])\n",
    "        \n",
    "        plt.plot(history.history['loss'], label='Training loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        predict_range = 1\n",
    "        train_dates = train_dates[0:train_sz]\n",
    "\n",
    "        n_future = pd.date_range(train_dates.iloc[look_back], periods = predict_range, freq = 'H').tolist()\n",
    "\n",
    "        prediction = model.predict(trainX[0:predict_range])\n",
    "\n",
    "        # col = 3\n",
    "\n",
    "        y_pred_future = scaler.inverse_transform(prediction)[:, 11]\n",
    "        print(y_pred_future)\n",
    "\n",
    "        #inverse boxcox\n",
    "        # y_pred_future = inv_boxcox(y_pred_future, fitted_lambda)\n",
    "        # y_pred_future = np.vectorize(lambda x: x - 1e-6)(y_pred_future)\n",
    "        # # y_pred_future = y_pred_future[0] - 1e-6\n",
    "\n",
    "        forecast_dates = []\n",
    "        for time_i in n_future:\n",
    "            forecast_dates.append(time_i.date())\n",
    "\n",
    "\n",
    "        df_forecast = pd.DataFrame({'date':np.array(forecast_dates), reading_types.at[int(typeId) - 1, 'reading_type_name']:y_pred_future})\n",
    "        df_forecast['date']=pd.to_datetime(df_forecast['date'])\n",
    "\n",
    "        original = df[['date', typeId]]\n",
    "        original['date']=pd.to_datetime(original['date'])\n",
    "        original = original[look_back:look_back+predict_range]\n",
    "\n",
    "        print(\"original\", '-'*80)\n",
    "        print(original.head(1))\n",
    "\n",
    "        print(\"df_forecast\", '-'*80)\n",
    "        print(df_forecast.head(1))\n",
    "\n",
    "        # sns.lineplot(data= original, x = 'date', y = typeId)\n",
    "        # sns.lineplot(data = df_forecast, x = 'date', y =  reading_types.at[int(typeId) - 1, 'reading_type_name'])\n",
    "        # plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
