{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit #for data preprocessing and crass validating \n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression #logistic Regression\n",
    "from sklearn.ensemble import RandomForestRegressor #Random Forest \n",
    "\n",
    "from statistics import mean\n",
    "from hyperopt import Trials, hp, fmin, tpe, STATUS_OK, space_eval #for hyperparameter tuning and minimizing\n",
    "\n",
    "from cyclic_boosting.pipelines import pipeline_CBClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "import itertools\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "from scipy.stats import boxcox \n",
    "from scipy.special import inv_boxcox\n",
    "\n",
    "from termcolor import colored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1832737 entries, 0 to 1832736\n",
      "Data columns (total 51 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   Unnamed: 0     int64         \n",
      " 1   date           datetime64[ns]\n",
      " 2   building_id    int64         \n",
      " 3   1              float64       \n",
      " 4   2              float64       \n",
      " 5   3              float64       \n",
      " 6   4              float64       \n",
      " 7   5              float64       \n",
      " 8   6              float64       \n",
      " 9   7              float64       \n",
      " 10  8              float64       \n",
      " 11  9              float64       \n",
      " 12  10             float64       \n",
      " 13  11             float64       \n",
      " 14  12             float64       \n",
      " 15  work_hours     bool          \n",
      " 16  day type       int64         \n",
      " 17  Fall           bool          \n",
      " 18  Spring         bool          \n",
      " 19  Summer         bool          \n",
      " 20  Winter         bool          \n",
      " 21  trimester_day  int64         \n",
      " 22  1_b            bool          \n",
      " 23  2_b            bool          \n",
      " 24  3_b            bool          \n",
      " 25  6_b            bool          \n",
      " 26  8_b            bool          \n",
      " 27  10_b           bool          \n",
      " 28  11_b           bool          \n",
      " 29  12_b           bool          \n",
      " 30  13_b           bool          \n",
      " 31  16_b           bool          \n",
      " 32  17_b           bool          \n",
      " 33  18_b           bool          \n",
      " 34  19_b           bool          \n",
      " 35  20_b           bool          \n",
      " 36  21_b           bool          \n",
      " 37  23_b           bool          \n",
      " 38  24_b           bool          \n",
      " 39  25_b           bool          \n",
      " 40  26_b           bool          \n",
      " 41  29_b           bool          \n",
      " 42  31_b           bool          \n",
      " 43  32_b           bool          \n",
      " 44  33_b           bool          \n",
      " 45  34_b           bool          \n",
      " 46  36_b           bool          \n",
      " 47  37_b           bool          \n",
      " 48  38_b           bool          \n",
      " 49  40_b           bool          \n",
      " 50  41_b           bool          \n",
      "dtypes: bool(34), datetime64[ns](1), float64(12), int64(4)\n",
      "memory usage: 297.1 MB\n"
     ]
    }
   ],
   "source": [
    "samples = pd.read_csv('preprocessed_lstm.csv')\n",
    "samples['date'] = pd.to_datetime(samples['date'])\n",
    "building_encoder = pd.get_dummies(samples['building_id'])\n",
    "samples = samples.join(building_encoder.add_suffix('_b'))\n",
    "reading_types = pd.read_csv('reading_types.csv')\n",
    "# samples.info()\n",
    "samples.info()\n",
    "\n",
    "\n",
    "df_lst = [(k, v) for k, v in samples.groupby('building_id')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building: 1 --------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[153], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWinter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpring\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummer\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork_hours\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrimester_day\u001b[39m\u001b[38;5;124m'\u001b[39m], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mto_pydatetime())\n\u001b[1;32m---> 22\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pydatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241m.\u001b[39mdayofweek\u001b[38;5;241m.\u001b[39mmap({\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;241m0\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;241m2\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;241m3\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;241m4\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;241m5\u001b[39m: \u001b[38;5;241m0\u001b[39m, \n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m6\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     30\u001b[0m })\n\u001b[0;32m     33\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseason\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mto_pydatetime()\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\u001b[38;5;241m.\u001b[39mmap({\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWinter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWinter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;241m12\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWinter\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     46\u001b[0m })\n\u001b[0;32m     48\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork_hours\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mto_pydatetime()\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mhour\u001b[38;5;241m.\u001b[39mbetween(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m18\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dt'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "value_type_ids = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
    "scaler = StandardScaler()  \n",
    "scaler = scaler.fit(samples[value_type_ids]) \n",
    "\n",
    "trainX, trainY, train_class = np.array([[[]]]).reshape(0, 10, 12), np.array([[]]).reshape(0, 12), np.array([[]]).reshape(0, 35)\n",
    "\n",
    "\n",
    "for building, df in df_lst:\n",
    "    print('building:', building, '-'*80)\n",
    "    train_dates = pd.to_datetime(df['date'])\n",
    "    df = df.set_index('date', drop = False) \n",
    "    idx = pd.date_range('01-01-2023', '12-31-2023 23:55', freq = '5min')\n",
    "    df = df.reindex(idx, fill_value = np.nan)\n",
    "\n",
    "    # df.info()\n",
    "    df = df.drop(['day type', 'Winter', 'Spring', 'Summer', 'Fall', 'work_hours', 'trimester_day'], axis = 1)\n",
    "\n",
    "    df['date'] = df.index\n",
    "    \n",
    "    df['day type'] = df['date'].dt.dayofweek.map({\n",
    "        0: 1,\n",
    "        1: 1,\n",
    "        2: 1,\n",
    "        3: 1,\n",
    "        4: 1,\n",
    "        5: 0, \n",
    "        6: 0\n",
    "    })\n",
    "\n",
    "        \n",
    "    df['season'] = df['date'].dt.month.map({\n",
    "        1: 'Winter',\n",
    "        2: 'Winter',\n",
    "        3: 'Spring',\n",
    "        4: 'Spring',\n",
    "        5: 'Spring',\n",
    "        6: 'Summer',\n",
    "        7: 'Summer',\n",
    "        8: 'Summer',\n",
    "        9: 'Fall',\n",
    "        10: 'Fall',\n",
    "        11: 'Fall',\n",
    "        12: 'Winter'\n",
    "    })\n",
    "\n",
    "    df['work_hours'] = df['date'].dt.hour.between(8, 18)\n",
    "    df['work_hours'].map({True: 1, False: 0})\n",
    "\n",
    "    df['Winter'] = df['season'].map({\n",
    "        'Winter': 1,\n",
    "        'Spring': 0,\n",
    "        'Summer': 0,\n",
    "        'Fall': 0\n",
    "    })\n",
    "    df['Spring'] = df['season'].map({\n",
    "        'Winter': 0,\n",
    "        'Spring': 1,\n",
    "        'Summer': 0,\n",
    "        'Fall': 0\n",
    "    })\n",
    "    df['Summer'] = df['season'].map({\n",
    "        'Winter': 0,\n",
    "        'Spring': 0,\n",
    "        'Summer': 1,\n",
    "        'Fall': 0\n",
    "    })\n",
    "    df['Fall'] = df['season'].map({\n",
    "        'Winter': 0,\n",
    "        'Spring': 0,\n",
    "        'Summer': 0,\n",
    "        'Fall': 1\n",
    "    })\n",
    "\n",
    "\n",
    "    df = df.drop('season', axis = 1)\n",
    "\n",
    "    multivariate = df.drop(['Unnamed: 0', 'building_id', 'date'], axis = 1)  \n",
    "    multivariate = multivariate.astype('float32')\n",
    "    classification_info = multivariate.drop(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], axis = 1)\n",
    "\n",
    "    classification_info.info()\n",
    "\n",
    "    multivariate = multivariate[['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']]\n",
    "    multivariate = multivariate.interpolate(method = 'linear').bfill().ffill()\n",
    "\n",
    "    multivariate.info()\n",
    "\n",
    "\n",
    "    # for typeId in value_type_ids:\n",
    "    #     if(multivariate[typeId].isnull().sum() <= len(multivariate) * 0.5):\n",
    "    #         print(typeId, \"gogogiaojsugdiadsug\")\n",
    "    #         print(df.info(show_counts = True))\n",
    "    #         multivariate[typeId] = multivariate[typeId].fillna(0)\n",
    "\n",
    "    sz = len(multivariate)\n",
    "    train_sz = int(sz * 0.9)\n",
    "    test_sz = len(multivariate) - train_sz\n",
    "\n",
    "    df_scaled = scaler.transform(multivariate)\n",
    "\n",
    "    train, test = df_scaled[0:train_sz,:], df_scaled[train_sz:sz,:]\n",
    "    tClass, tstClass = classification_info[0: train_sz], classification_info[train_sz:sz]\n",
    "\n",
    "    test_dates = train_dates[train_sz:sz]\n",
    "\n",
    "    def create_dataset(dataset, classes, look_back = 1):\n",
    "        dataX, dataY = [], []\n",
    "        class_X = []\n",
    "        for i in range(len(dataset) - look_back - 1):\n",
    "            if(np.isnan(dataset[i + look_back]).any()): \n",
    "                continue\n",
    "            dataY.append(dataset[i + look_back, :])\n",
    "            a = dataset[i:(i + look_back), :]\n",
    "            if(np.isnan(a).any()): # for masking\n",
    "                a.fill(-1)\n",
    "            dataX.append(a)\n",
    "            class_X.append(classes.iloc[i + look_back, :])\n",
    "\n",
    "        return np.array(dataX), np.array(dataY), np.array(class_X)\n",
    "    \n",
    "    look_back = 10\n",
    "    tX, tY, tClass = create_dataset(train, tClass, look_back)\n",
    "\n",
    "    print(tX.shape, tY.shape, tClass.shape)\n",
    "\n",
    "    # testX, testY, test_class = create_dataset(test, test_class, look_back)\n",
    "    \n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    tX, tY, tClass = np.array(tX), np.array(tY), np.array(tClass)\n",
    "\n",
    "    print(tX.shape, tY.shape, tClass.shape, trainX.shape, trainY.shape, train_class.shape)\n",
    "    trainX, trainY, train_class = np.append(trainX, tX, axis = 0), np.append(trainY, tY, axis = 0), np.append(train_class, tClass, axis = 0)\n",
    "    # testX, testY, test_class = np.array(testX), np.array(testY), np.array(test_class)\n",
    "\n",
    "    # print('trainX shape == {}.'.format(trainX.shape))\n",
    "    # print('trainY shape == {}.'.format(trainY.shape)) \n",
    "    # print('train_class shape == {}.'.format(train_class.shape))\n",
    "    \n",
    "    # plt.figure(figsize = (18, 10))\n",
    "    # plt.plot(history.history['loss'], label='Training loss')\n",
    "    # plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    # plt.clf()\n",
    "    # plt.cla()\n",
    "    # plt.close()\n",
    "\n",
    "\n",
    "    # predict_range = 1\n",
    "    # train_dates = train_dates[0:train_sz]\n",
    "\n",
    "    # n_future = pd.date_range(train_dates.iloc[look_back], periods = predict_range, freq = '5min').tolist()\n",
    "\n",
    "    # prediction = model.predict(trainX[look_back:look_back + predict_range])\n",
    "\n",
    "    # col = 3\n",
    "\n",
    "    # y_pred_future = scaler.inverse_transform(prediction)[:, 10]\n",
    "    # print(y_pred_future)\n",
    "\n",
    "    # #inverse boxcox\n",
    "    # # y_pred_future = inv_boxcox(y_pred_future, fitted_lambda)\n",
    "    # # y_pred_future = np.vectorize(lambda x: x - 1e-6)(y_pred_future)\n",
    "    # # # y_pred_future = y_pred_future[0] - 1e-6\n",
    "\n",
    "    # forecast_dates = []\n",
    "    # for time_i in n_future:\n",
    "    #     forecast_dates.append(time_i.date())\n",
    "\n",
    "\n",
    "    # df_forecast = pd.DataFrame({'date':np.array(forecast_dates), reading_types.at[int('11') - 1, 'reading_type_name']:y_pred_future})\n",
    "    # df_forecast['date']=pd.to_datetime(df_forecast['date'])\n",
    "\n",
    "    # original = df[['date', '11']]\n",
    "    # original['date']=pd.to_datetime(original['date'])\n",
    "    # original = original[look_back:look_back+predict_range]\n",
    "\n",
    "    # print(\"original\", '-'*80)\n",
    "    # print(original.head(1))\n",
    "\n",
    "    # print(\"df_forecast\", '-'*80)\n",
    "    # print(df_forecast.head(1))\n",
    "    # print(reading_types.at[int(typeId) - 1, 'reading_type_name'], '='*100)\n",
    "\n",
    "    # # print(trainX[-predict_range:])\n",
    "    # # print(df[train_sz-predict_range:train_sz])\n",
    "    # # print(df_forecast)\n",
    "    # # print(original)\n",
    "    # # print(\"original\", '-'*80)\n",
    "    # # print(original.head(24))\n",
    "\n",
    "    # # print(\"df_forecast\", '-'*80)\n",
    "    # # print(df_forecast.head(24))\n",
    "\n",
    "    # # original.set_index('date')\n",
    "    # # df_forecast.set_index('date')\n",
    "\n",
    "    # print(colored(\"MEAN SQUARED ERROR: \", 'red'), mean_squared_error(original[typeId], df_forecast[reading_types.at[int(typeId) - 1, 'reading_type_name']]))\n",
    "\n",
    "    # # plt.figure(figsize=(18,8))\n",
    "    # # plt.plot(original[typeId],label = \"original\")\n",
    "    # # plt.plot(df_forecast[reading_types.at[int(typeId) - 1, 'reading_type_name']],label = \"predicted\")\n",
    "    # # plt.title(\"Time Series Forecast\")\n",
    "    # # plt.xlabel(\"Date\")\n",
    "    # # plt.ylabel(reading_types.at[int(typeId) - 1, 'reading_type_name'])\n",
    "    # # plt.legend()\n",
    "    # # plt.show()\n",
    "\n",
    "    # sns.lineplot(data= original, x = 'date', y = typeId)\n",
    "    # sns.lineplot(data = df_forecast, x = 'date', y =  reading_types.at[int(typeId) - 1, 'reading_type_name'])\n",
    "    # plt.show()\n",
    "\n",
    "print(trainX.shape, trainY.shape, train_class.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34292/34292 [==============================] - 1386s 40ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/20\n",
      " 2730/34292 [=>............................] - ETA: 18:17 - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[143], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(), loss \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mHuber() } )\n\u001b[1;32m---> 21\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_class\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainY\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "value_input = keras.Input(shape = (trainX.shape[1], trainX.shape[2]), name = \"values\")\n",
    "lstm1_value = layers.LSTM(150, return_sequences = True)(value_input)\n",
    "lstm2_value = layers.LSTM(50, return_sequences = False)(lstm1_value)\n",
    "# bn_value = layers.BatchNormalization()(lstm2_value)\n",
    "# output_value = layers.Dense(1, activation = 'linear')(bn_value)\n",
    "\n",
    "classification_input = keras.Input(shape = (train_class.shape[1],), name = \"class\")\n",
    "\n",
    "x = layers.concatenate([lstm2_value, classification_input])\n",
    "dense_layer = layers.Dense(30, activation = 'relu')(x)\n",
    "# batch_norm = layers.BatchNormalization()(x)\n",
    "val_pred = layers.Dense(1, activation = 'linear', name = \"output\")(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs = [value_input, classification_input], outputs = val_pred)\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor = 'val_loss', patience = 5)\n",
    "\n",
    "model.compile(optimizer= tf.keras.optimizers.Adam(), loss = {\"output\": tf.keras.losses.Huber() } )\n",
    "\n",
    "history = model.fit({\n",
    "    \"values\": trainX, \n",
    "    \"class\": train_class\n",
    "}, {\n",
    "    \"output\": trainY\n",
    "}, batch_size = 64, validation_split = 0.2, epochs = 20, verbose = 1, callbacks = [early_stop])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
