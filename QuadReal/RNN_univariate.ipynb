{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jonat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit #for data preprocessing and crass validating \n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression #logistic Regression\n",
    "from sklearn.ensemble import RandomForestRegressor #Random Forest \n",
    "\n",
    "from statistics import mean\n",
    "from hyperopt import Trials, hp, fmin, tpe, STATUS_OK, space_eval #for hyperparameter tuning and minimizing\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "import itertools\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "from scipy.stats import boxcox \n",
    "from scipy.special import inv_boxcox\n",
    "\n",
    "from termcolor import colored\n",
    "\n",
    "import joblib\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_csv('preprocessed_lstm.csv')\n",
    "samples['date'] = pd.to_datetime(samples['date'])\n",
    "reading_types = pd.read_csv('reading_types.csv')\n",
    "# samples.info()\n",
    "\n",
    "df_lst = [(k, v) for k, v in samples.groupby('building_id')]\n",
    "\n",
    "models = [[]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ================================================================================\n",
      "1 ================================================================================\n",
      "WARNING:tensorflow:From c:\\Users\\jonat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From c:\\Users\\jonat\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "1008/1008 [==============================] - 32s 26ms/step - loss: 0.1420 - val_loss: 0.0814\n",
      "Epoch 2/20\n",
      "1006/1008 [============================>.] - ETA: 0s - loss: 0.1357"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "value_type_ids = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
    "preds = pd.DataFrame()\n",
    "cnt = 0\n",
    "modelDict = {}\n",
    "for building, df in df_lst[cnt:]:\n",
    "    modelDict[building] = []\n",
    "    print(building, '=' * 80)\n",
    "    df = df.set_index('date', drop = False) \n",
    "    idx = pd.date_range('01-01-2023', '12-31-2023 23:55', freq = '5min')\n",
    "    df = df.reindex(idx, fill_value = np.nan)\n",
    "    for typeId in value_type_ids:\n",
    "        print(typeId, '=' * 80)\n",
    "        multivariate = df.drop(['Unnamed: 0', 'building_id', 'date'], axis = 1)  \n",
    "        multivariate = multivariate.astype('float32')\n",
    "        multivariate = multivariate[[typeId]]\n",
    "        # print(multivariate.info())\n",
    "        sz = len(multivariate)\n",
    "        train_sz = int(sz * 0.9)\n",
    "        test_sz = len(multivariate) - train_sz\n",
    "\n",
    "        scaler = StandardScaler()  \n",
    "        scaler = scaler.fit(multivariate) \n",
    "        df_scaled = scaler.transform(multivariate)\n",
    "\n",
    "        train, test = df_scaled[:train_sz,:], df_scaled[train_sz:sz,:]\n",
    "\n",
    "        def create_dataset(dataset, look_back = 1):\n",
    "            dataX, dataY = [], []\n",
    "            for i in range(len(dataset) - look_back - 1):\n",
    "                if(np.isnan(dataset[i + look_back])): \n",
    "                    continue\n",
    "                a = dataset[i:(i + look_back), :]\n",
    "                if(np.isnan(a).any()): # for masking\n",
    "                    a.fill(-1)\n",
    "\n",
    "                dataX.append(a)\n",
    "                dataY.append(dataset[i + look_back])\n",
    "            return np.array(dataX), np.array(dataY)\n",
    "        \n",
    "        look_back = 20\n",
    "        trainX, trainY = create_dataset(train, look_back)\n",
    "        testX, testY = create_dataset(test, look_back)\n",
    "        \n",
    "        # reshape input to be [samples, time steps, features]\n",
    "        trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "        testX, testY = np.array(testX), np.array(testY)\n",
    "\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Masking(mask_value = -1, input_shape = (trainX.shape[1], trainX.shape[2])))\n",
    "        model.add(layers.LSTM(150, input_shape = (trainX.shape[1], trainX.shape[2]), return_sequences = True))\n",
    "        model.add(LSTM(50, return_sequences = False))\n",
    "        # model.add(LSTM(100))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dense(1, activation= 'linear'))\n",
    "\n",
    "\n",
    "        early_stop = EarlyStopping(monitor = 'val_loss', patience = 3)\n",
    "\n",
    "        model.compile(optimizer= tf.keras.optimizers.Adam(), loss= tf.keras.losses.Huber())\n",
    "\n",
    "        history = model.fit(trainX, trainY, batch_size=64, validation_split = 0.2, epochs=20, verbose = 1, callbacks = [early_stop])\n",
    "        \n",
    "        # plt.figure(figsize = (18, 10))\n",
    "        # plt.plot(history.history['loss'], label='Training loss')\n",
    "        # plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "        # plt.clf()\n",
    "        # plt.cla()\n",
    "        # plt.close()\n",
    "\n",
    "        # predict_range = 24\n",
    "        # test_dates = test_dates[0:predict_range]\n",
    "\n",
    "        # n_future = pd.date_range(test_dates.iloc[0], periods = predict_range, freq = 'H').tolist()\n",
    "\n",
    "        # prediction = model.predict(trainX[-predict_range:])\n",
    "        # prevVal = -1 \n",
    "\n",
    "        # def get_predictions(val): \n",
    "        #     global prevVal \n",
    "\n",
    "        #     progress_bar.update(1)\n",
    "        #     if(np.isnan(val)): \n",
    "        #         prevVal_reshaped = np.array([[prevVal]], dtype=np.float32)\n",
    "        #         pred = model.predict(prevVal_reshaped, verbose = 0)\n",
    "        #         pred = scaler.inverse_transform(pred)[:, 0][0]\n",
    "        #     else: \n",
    "        #         pred = val\n",
    "            \n",
    "        #     prevVal = pred\n",
    "        #     return pred \n",
    " \n",
    "        # progress_bar = tqdm(total = len(multivariate))\n",
    "        # df[typeId] = df[typeId].apply(get_predictions)\n",
    "        # df.to_csv('dfoutput.csv')\n",
    "        modelDict[building].append(model)\n",
    "    # preds = preds.append(df)\n",
    "    # preds.to_csv('output.csv')\n",
    "    # cnt += 1\n",
    "    # print(cnt, '+' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modelDict.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(modelDict, 'modelDict2.pkl', compress = 9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
