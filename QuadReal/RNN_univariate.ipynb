{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit #for data preprocessing and crass validating \n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression #logistic Regression\n",
    "from sklearn.ensemble import RandomForestRegressor #Random Forest \n",
    "\n",
    "from statistics import mean\n",
    "from hyperopt import Trials, hp, fmin, tpe, STATUS_OK, space_eval #for hyperparameter tuning and minimizing\n",
    "\n",
    "from cyclic_boosting.pipelines import pipeline_CBClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import MeanSquaredError\n",
    "\n",
    "import itertools\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "from scipy.stats import boxcox \n",
    "from scipy.special import inv_boxcox\n",
    "\n",
    "from termcolor import colored\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.read_csv('preprocessed_lstm.csv')\n",
    "samples['date'] = pd.to_datetime(samples['date'])\n",
    "reading_types = pd.read_csv('reading_types.csv')\n",
    "# samples.info()\n",
    "\n",
    "df_lst = [(k, v) for k, v in samples.groupby('building_id')]\n",
    "\n",
    "models = [[]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ================================================================================\n",
      "Epoch 1/20\n",
      "93/93 [==============================] - 8s 34ms/step - loss: 0.2396 - val_loss: 0.2988\n",
      "Epoch 2/20\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 0.1880 - val_loss: 0.2926\n",
      "Epoch 3/20\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 0.1792 - val_loss: 0.2796\n",
      "Epoch 4/20\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 0.1781 - val_loss: 0.2506\n",
      "Epoch 5/20\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 0.1738 - val_loss: 0.2202\n",
      "Epoch 6/20\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 0.1780 - val_loss: 0.2000\n",
      "Epoch 7/20\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 0.1789 - val_loss: 0.1622\n",
      "Epoch 8/20\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 0.1742 - val_loss: 0.1518\n",
      "Epoch 9/20\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 0.1745 - val_loss: 0.1380\n",
      "Epoch 10/20\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 0.1768 - val_loss: 0.1500\n",
      "Epoch 11/20\n",
      "93/93 [==============================] - 2s 24ms/step - loss: 0.1725 - val_loss: 0.1341\n",
      "Epoch 12/20\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 0.1745 - val_loss: 0.1347\n",
      "Epoch 13/20\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 0.1745 - val_loss: 0.1416\n",
      "Epoch 14/20\n",
      "93/93 [==============================] - 3s 29ms/step - loss: 0.1743 - val_loss: 0.1321\n",
      "Epoch 15/20\n",
      "93/93 [==============================] - 3s 30ms/step - loss: 0.1723 - val_loss: 0.1587\n",
      "Epoch 16/20\n",
      "93/93 [==============================] - 3s 27ms/step - loss: 0.1701 - val_loss: 0.1509\n",
      "Epoch 17/20\n",
      "93/93 [==============================] - 2s 24ms/step - loss: 0.1735 - val_loss: 0.1413\n",
      "Epoch 18/20\n",
      "93/93 [==============================] - 2s 24ms/step - loss: 0.1703 - val_loss: 0.1346\n",
      "Epoch 19/20\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 0.1740 - val_loss: 0.1403\n",
      "Epoch 20/20\n",
      "93/93 [==============================] - 2s 23ms/step - loss: 0.1717 - val_loss: 0.1349\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m predict_range \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m\n\u001b[0;32m     77\u001b[0m test_dates \u001b[38;5;241m=\u001b[39m test_dates[\u001b[38;5;241m0\u001b[39m:predict_range]\n\u001b[1;32m---> 79\u001b[0m n_future \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mdate_range(\u001b[43mtest_dates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, periods \u001b[38;5;241m=\u001b[39m predict_range, freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     81\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(trainX[\u001b[38;5;241m-\u001b[39mpredict_range:])\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# col = 3\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1714\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1713\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m-> 1714\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexing.py:1647\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1645\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "value_type_ids = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
    "modelDict = {}\n",
    "for building, df in df_lst:\n",
    "    modelDict[building] = []\n",
    "    for typeId in value_type_ids:\n",
    "        print(building, '=' * 80)\n",
    "        train_dates = pd.to_datetime(df['date'])\n",
    "        multivariate = df.drop(['Unnamed: 0', 'building_id', 'date'], axis = 1)  \n",
    "        # multivariate.info()\n",
    "        multivariate = multivariate.astype('float32')\n",
    "        multivariate = multivariate[[typeId]]\n",
    "\n",
    "        # boxcox\n",
    "        # multivariate[typeId] = multivariate[typeId].apply(lambda x: x + 1e-6)\n",
    "        # df_boxcox_vals, fitted_lambda = boxcox(multivariate[typeId])\n",
    "        # multivariate[typeId] = df_boxcox_vals\n",
    "\n",
    "        sz = len(multivariate)\n",
    "        train_sz = int(sz * 0.9)\n",
    "        test_sz = len(multivariate) - train_sz\n",
    "\n",
    "        scaler = StandardScaler()  \n",
    "        scaler = scaler.fit(multivariate) \n",
    "        df_scaled = scaler.transform(multivariate)\n",
    "\n",
    "        train, test = df_scaled[0:train_sz,:], df_scaled[train_sz:sz,:]\n",
    "        test_dates = train_dates[train_sz:sz]\n",
    "\n",
    "        def create_dataset(dataset, look_back = 1):\n",
    "            dataX, dataY = [], []\n",
    "            for i in range(len(dataset) - look_back - 1):\n",
    "                a = dataset[i:(i + look_back), :]\n",
    "                dataX.append(a)\n",
    "                dataY.append(dataset[i + look_back, :])\n",
    "            return np.array(dataX), np.array(dataY)\n",
    "        \n",
    "        look_back = 10\n",
    "        trainX, trainY = create_dataset(train, look_back)\n",
    "        testX, testY = create_dataset(test, look_back)\n",
    "        \n",
    "        # reshape input to be [samples, time steps, features]\n",
    "        trainX, trainY = np.array(trainX), np.array(trainY)\n",
    "        testX, testY = np.array(testX), np.array(testY)\n",
    "\n",
    "        # print('trainX shape == {}.'.format(trainX.shape))\n",
    "        # print('trainY shape == {}.'.format(trainY.shape)) \n",
    "\n",
    "        # x_train, x_test, y_train, y_test = train_test_split(df.drop(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], axis = 1), df[['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']])\n",
    "        # x_train = np.asarray(x_train).astype(np.float32)\n",
    "        # y_train = np.asarray(y_train).astype(np.float32)\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.LSTM(150,input_shape = (trainX.shape[1], trainX.shape[2]), return_sequences = True))\n",
    "        model.add(LSTM(50, return_sequences = False))\n",
    "        # model.add(LSTM(100))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dense(1, activation= 'linear'))\n",
    "\n",
    "\n",
    "        early_stop = EarlyStopping(monitor = 'val_loss', patience = 6)\n",
    "\n",
    "        model.compile(optimizer= tf.keras.optimizers.Adam(), loss= tf.keras.losses.Huber())\n",
    "\n",
    "        history = model.fit(trainX, trainY, batch_size=64, validation_split = 0.2, epochs=20, verbose = 1, callbacks = [early_stop])\n",
    "        \n",
    "        # plt.figure(figsize = (18, 10))\n",
    "        # plt.plot(history.history['loss'], label='Training loss')\n",
    "        # plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "        # plt.clf()\n",
    "        # plt.cla()\n",
    "        # plt.close()\n",
    "\n",
    "        predict_range = 24\n",
    "        test_dates = test_dates[0:predict_range]\n",
    "\n",
    "        n_future = pd.date_range(test_dates.iloc[0], periods = predict_range, freq = 'H').tolist()\n",
    "\n",
    "        prediction = model.predict(trainX[-predict_range:])\n",
    "\n",
    "        # col = 3\n",
    "\n",
    "        y_pred_future = scaler.inverse_transform(prediction)[:, 0]\n",
    "        # print(y_pred_future)\n",
    "\n",
    "        #inverse boxcox\n",
    "        # y_pred_future = inv_boxcox(y_pred_future, fitted_lambda)\n",
    "        # y_pred_future = np.vectorize(lambda x: x - 1e-6)(y_pred_future)\n",
    "        # # y_pred_future = y_pred_future[0] - 1e-6\n",
    "\n",
    "        forecast_dates = []\n",
    "        for time_i in n_future:\n",
    "            forecast_dates.append(time_i)\n",
    "\n",
    "\n",
    "        df_forecast = pd.DataFrame({'date':np.array(forecast_dates), reading_types.at[int(typeId) - 1, 'reading_type_name']:y_pred_future})\n",
    "        df_forecast['date']=pd.to_datetime(df_forecast['date'])\n",
    "\n",
    "        original = df[['date', typeId]]\n",
    "        original['date']=pd.to_datetime(original['date'])\n",
    "        original = original[train_sz:train_sz + predict_range]\n",
    "\n",
    "        # print(reading_types.at[int(typeId) - 1, 'reading_type_name'], '='*100)\n",
    "\n",
    "        # # print(trainX[-predict_range:])\n",
    "        # # print(df[train_sz-predict_range:train_sz])\n",
    "        # # print(df_forecast)\n",
    "        # # print(original)\n",
    "        # # print(\"original\", '-'*80)\n",
    "        # # print(original.head(24))\n",
    "\n",
    "        # # print(\"df_forecast\", '-'*80)\n",
    "        # # print(df_forecast.head(24))\n",
    "\n",
    "        # # original.set_index('date')\n",
    "        # # df_forecast.set_index('date')\n",
    "\n",
    "        # print(colored(\"MEAN SQUARED ERROR: \", 'red'), mean_squared_error(original[typeId], df_forecast[reading_types.at[int(typeId) - 1, 'reading_type_name']]))\n",
    "\n",
    "        # # plt.figure(figsize=(18,8))\n",
    "        # # plt.plot(original[typeId],label = \"original\")\n",
    "        # # plt.plot(df_forecast[reading_types.at[int(typeId) - 1, 'reading_type_name']],label = \"predicted\")\n",
    "        # # plt.title(\"Time Series Forecast\")\n",
    "        # # plt.xlabel(\"Date\")\n",
    "        # # plt.ylabel(reading_types.at[int(typeId) - 1, 'reading_type_name'])\n",
    "        # # plt.legend()\n",
    "        # # plt.show()\n",
    "\n",
    "        # sns.lineplot(data= original, x = 'date', y = typeId)\n",
    "        # sns.lineplot(data = df_forecast, x = 'date', y =  reading_types.at[int(typeId) - 1, 'reading_type_name'])\n",
    "        # plt.show()\n",
    "        modelDict[building].append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modelDict.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(modelDict, 'modelDict.pkl', compress = 9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
